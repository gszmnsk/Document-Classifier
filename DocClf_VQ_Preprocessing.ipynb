{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "import scipy.cluster.vq as vq\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "data_directory = 'C:\\\\Users\\i\\Documents\\Python Scripts\\SPM_docs\\docs\\docs_path'\n",
    "categories = ['News', 'Resume', 'Scientific']\n",
    "\n",
    "data = []\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "def load_data():\n",
    "    '''\n",
    "    Loading the data by the category, converting jpg into array.\n",
    "    Output:\n",
    "    X - images dataset\n",
    "    y - labels\n",
    "    '''\n",
    "    img_height = 1000\n",
    "    img_width = 800\n",
    "\n",
    "    for category in categories:\n",
    "        path = os.path.join(data_directory, category)\n",
    "        class_number = categories.index(category)\n",
    "        \n",
    "        for img in os.listdir(path): \n",
    "            try:\n",
    "                image_array = cv2.imread(os.path.join(path, img))\n",
    "                resized_image_array = cv2.resize(image_array, (img_height, img_width))\n",
    "                data.append([resized_image_array, class_number])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "    random.shuffle(data)\n",
    "    \n",
    "    for features, label in data:\n",
    "        X.append(features) \n",
    "        y.append(label)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "def denoising_images(image_path):\n",
    "    \"\"\"\n",
    "    Denoising for better generalization.\n",
    "    \"\"\"\n",
    "\n",
    "    thresh = cv2.threshold(image_path, 220, 255, cv2.THRESH_BINARY_INV)[1]\n",
    "    \n",
    "    kernel1 = cv2.getStructuringElement(cv2.MORPH_RECT, (2,2))\n",
    "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel1)\n",
    "    \n",
    "    result = 255 - opening\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def ORB_extractor(X):\n",
    "    \n",
    "    '''\n",
    "    Extracting interesting points from images.\n",
    "    \n",
    "    Input: \n",
    "    X - list of image dataset\n",
    "    Output:\n",
    "    descriptors_scaled - array containing scaled descriptors\n",
    "    descriptors_list - array containing images and corresponding set of descriptors\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #if type(X) is list:\n",
    "        #pass\n",
    "    #else:\n",
    "        #X = [X]\n",
    "    \n",
    "    descriptors_list = []\n",
    "    orb = cv2.ORB_create(nfeatures = 1200)\n",
    "\n",
    "    for image in X:\n",
    "        keypoint = orb.detect(image,None)\n",
    "        keypoints, descriptor = orb.compute(image, keypoint)\n",
    "        descriptors_list.append((image, descriptor))\n",
    "    \n",
    "    descriptors = descriptors_list[0][1]\n",
    "    for image_path, descriptor in descriptors_list[1:]:\n",
    "        descriptors = np.vstack((descriptors,descriptor))\n",
    "        \n",
    "    standard_scaler = StandardScaler().fit(descriptors)\n",
    "    descriptors_scaled = standard_scaler.transform(descriptors)\n",
    "    \n",
    "    return descriptors_scaled, descriptors_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_codebook(descriptors, voc_size):\n",
    "    \"\"\"\n",
    "    Creating the codebook, vocabulary.\n",
    "    \n",
    "    Inupt: \n",
    "    descriptors - scaled array of descriptors\n",
    "    voc_size  - vocabulary size (the number of desired clusters)\n",
    "    Output:\n",
    "    codebook - squeezed centers of descriptor clusters\n",
    "    \"\"\"\n",
    "    features  = np.vstack((descriptor for descriptor in descriptors))\n",
    "    kmedoids = KMedoids(n_clusters=voc_size,  init='k-medoids++', random_state = 0).fit(features)\n",
    "    y_kmed = kmedoids.fit_predict(features)\n",
    "    codebook = kmedoids.cluster_centers_.squeeze()\n",
    "    return codebook\n",
    "\n",
    "\n",
    "\n",
    "def vector_encoder(X, descriptors_list, codebook, voc_size):\n",
    "    '''\n",
    "    Vector Quantization\n",
    "    \n",
    "    Input: \n",
    "    X - image dataset\n",
    "    descriptors_list - array: images and corresponding list of its descriptors\n",
    "    codebook,\n",
    "    voc_size  - vocabulary size (the number of desired clusters)\n",
    "    Output:\n",
    "    image_features - list of compressed images\n",
    "    '''\n",
    "    image_features = np.zeros((len(X),voc_size), \"float32\")\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        words, distance = vq.vq(descriptors_list[i][1], codebook)    \n",
    "        \n",
    "        for w in words:\n",
    "            image_features[i][w] += 1 \n",
    "    \n",
    "    standard_scaler = StandardScaler().fit(image_features)\n",
    "    image_features = standard_scaler.transform(image_features)\n",
    "    \n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-d89ad6b5ff1e>:114: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  features  = np.vstack((descriptor for descriptor in descriptors))\n"
     ]
    }
   ],
   "source": [
    "X = load_data()[0]\n",
    "y = load_data()[1]\n",
    "X_codeboook = X[:40]\n",
    "X_codebook_denoised = [denoising_images(img) for img in X_codeboook]\n",
    "\n",
    "X_codebook_descriptors = ORB_extractor(X_codebook_denoised)[0]\n",
    "\n",
    "codebook = build_codebook(X_codebook_descriptors,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json, codecs\n",
    "list_codebook = codebook.tolist() # nested lists with same data, indices\n",
    "with codecs.open('codebook.json', 'w', encoding='utf-8') as handle:\n",
    "        json.dump(list_codebook, handle, ensure_ascii=False, separators=(',', ':'), sort_keys=True, indent=4)\n",
    "\n",
    "list_codebook_new = json.loads(codecs.open('codebook.json', 'r', encoding='utf-8').read())\n",
    "_codebook = np.array(list_codebook_new)\n",
    "\n",
    "# source: https://stackoverflow.com/questions/12309269/how-do-i-write-json-data-to-a-file\n",
    "# https://stackoverflow.com/questions/26646362/numpy-array-is-not-json-serializable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_denoised = [denoising_images(img) for img in X]\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_denoised, y, train_size = 0.6, random_state=42)\n",
    "\n",
    "\n",
    "descriptors_list_train = ORB_extractor(X_train)[1]\n",
    "descriptors_list_test = ORB_extractor(X_test)[1]\n",
    "\n",
    "\n",
    "vector_encoder_train = vector_encoder(X_train, descriptors_list_train, codebook, 15) \n",
    "vector_encoder_test = vector_encoder(X_test, descriptors_list_test, codebook, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_X_train = X_train[0].tolist() # nested lists with same data, indices\n",
    "with codecs.open('X_train.json', 'w', encoding='utf-8') as handle:\n",
    "        json.dump(_X_train, handle, ensure_ascii=False, separators=(',', ':'), sort_keys=True, indent=4)\n",
    "_X_test = X_test[0].tolist() # nested lists with same data, indices\n",
    "with codecs.open('X_test.json', 'w', encoding='utf-8') as handle:\n",
    "        json.dump(_X_test, handle, ensure_ascii=False, separators=(',', ':'), sort_keys=True, indent=4)\n",
    "\n",
    "with codecs.open('y_train.json', 'w', encoding='utf-8') as handle:\n",
    "        json.dump(y_train, handle, ensure_ascii=False, separators=(',', ':'), sort_keys=True, indent=4)\n",
    "with codecs.open('y_test.json', 'w', encoding='utf-8') as handle:\n",
    "        json.dump(y_test, handle, ensure_ascii=False, separators=(',', ':'), sort_keys=True, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "_vector_encoder_train = vector_encoder_train.tolist() # nested lists with same data, indices\n",
    "with codecs.open('vector_encoder_train.json', 'w', encoding='utf-8') as handle:\n",
    "        json.dump(_vector_encoder_train, handle, ensure_ascii=False, separators=(',', ':'), sort_keys=True, indent=4)\n",
    "\n",
    "_vector_encoder_test = vector_encoder_test.tolist() # nested lists with same data, indices\n",
    "with codecs.open('vector_encoder_test.json', 'w', encoding='utf-8') as handle:\n",
    "        json.dump(_vector_encoder_test, handle, ensure_ascii=False, separators=(',', ':'), sort_keys=True, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}